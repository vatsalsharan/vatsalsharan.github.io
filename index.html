<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/starter-template.css" rel="stylesheet">

    <title>Vatsal Sharan</title>

  </head>

  <body>
    
    <div class="container">

    	<div class="row">
            <div class="col-lg-12">
                <h1 class="page-header">Vatsal Sharan
                </h1>
            </div>
        </div>

      <div class="row">
        <div class="col-md-2">
		<div class="photo">
	  		<img id="picture" src="docs/vatsal.jpg" style="width:240px;height:280px;" class="photo_pic"/>
		</div>      
		</div>
        <div class="col-md-5 col-md-offset-1">
          <br>
		  <font size="3">
		  <p>Assistant Professor,<br>
		  Thomas Lord Department of Computer Science,<br>
          University of Southern California <br>
		  </p>
		  <p>
          Email: vsharan at usc dot edu
		  </p>
          <p>
          Office: GCS 402N
          </p>
		  </font>
		</div>
      </div>
      <div class="voffset1"></div>
      <div class="row">
        <div class="span12">
		<section id="about">

	    <h3>About</h3>

			<p>
            
            
            I'm an assistant professor of computer science at USC. I am a part of the <a href="https://viterbi-web.usc.edu/~cstheory/">Theory Group</a>, <a href="https://mascle.usc.edu/">Machine Learning Center</a> and the <a href="https://live-usc-cais.pantheonsite.io/">Center for AI in Society</a> at USC.
 Previously, I was a postdoc at MIT hosted by <a href="https://people.csail.mit.edu/moitra/">Ankur Moitra</a>. I obtained my Ph.D. from Stanford advised by <a href="http://theory.stanford.edu/~valiant/">Greg Valiant</a>. <br><br>
            
            <!--I work on the foundations of machine learning, and my interests mostly lie in the intersection of machine learning, theoretical computer science and statistics. My research aims to understand fundamental limits for solving learning and estimation tasks given computational and information theoretic constraints, and to use this understanding to develop practical algorithms which are efficient, fair and robust. If you're interested in learning more, here are some questions I've recently been working on:
            -->
            
            I work on the foundations of machine learning, and my interests mostly lie in the intersection of machine learning, theoretical computer science and statistics. The goal of my research is to study and discover the underlying principles which govern learning, and to leverage this understanding to build practical machine learning systems which are more efficient, fair and robust. A large part of my work aims to inspect questions which arise from modern applications and challenges of machine learning. If you're interested in learning more, some of my representative work is highlighted below.
            
            
            <!--If you're interested in learning more, here are some questions I've recently been working on:
            
            <ul>
                <li>What is the role of computational and statistical constraints in learning and optimization? Are there inherent trade-offs between the amount of memory required for learning or optimization, and the amount of data or computation required? How do we solve learning tasks  with limited data, such as by designing optimal data augmentation and regularization techniques?
                <li>What are appropriate notions of fairness in various domains, and how do we train models which respect these notions? Similarly, how do we ensure trained models are robust, such as when evaluated on data distributions which differ from the original training distribution?
                <li>How can we understand deep neural networks and foundation models in the context of some of the above considerations?
            </ul>
            -->
            
            <br><br>
            
            
             My research is supported by the NSF, Amazon Research and Google Research. This support is very gratefully acknowledged.<br><br>

        
            
            <!-- <div style="font-size:15px;font-weight:bold">I'm looking to take Ph.D. students for Fall 2022. If interested, please apply to the CS Ph.D. program at USC and mention my name in your application. I especially encourage members of underrepresented groups in computer science to apply. USC offers some <a href="https://gradadm.usc.edu/lightboxes/us-students-fee-waivers/">fee waivers </a> for eligible applicants.


            
            <br></div>-->
            
            I'm also a part of the <a href="https://let-all.com/">Learning Theory Alliance (LeT-All)</a>, a community building and mentorship initiative for the learning theory community. <br><br>

            Here is my <a href="docs/Vatsal-Resume.pdf">CV</a>, which has a more complete list of activities.
		</section>

      


	  <section id="papers">
	    <h3>Some Representative Publications | <a href="https://vatsalsharan.github.io/pubs.html"><b>All Publications</b></a> </h3>
            
            <div style="height: 12px;"></div>
            
            <h4><em>Using algorithms to understand Transformers, and using Transformers to understand algorithms</em></h4>
            
            <div style="height: 8px;"></div>
           
     Theory can provide a bird's eye view of the landscape of information, computation and how they interact for learning problems. Can we use some of this computational and information theoretic understanding to understand Transformers? On the flip side, can we use Transformers to explore this landscape, and understand and discover algorithms and data structures?
     
     <div style="height: 8px;"></div>
     
     Here is a <a href="https://www.youtube.com/watch?v=qHWBk5ewYwI"><b>talk</b></a> which covers some of this work (<a href="docs/vatsal_simons.pdf"><b>slides</b></a>).

            <br>
            
            <ul>
            
            <li><p> <div style="font-size:15px;font-weight:bold;">
                Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression
              </div>
                <div style="margin-left: 15px;">
                Deqing Fu, Tian-Qi Chen, Robin Jia, Vatsal Sharan
            <br>
            <i>Neurips, 2024</i><br>
            <i><b> SoCal NLP Symposium 2023 Best Paper Award </b></i><br>
            <a href="https://arxiv.org/abs/2310.17086"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2310.17086.pdf"><b>pdf</b></a>
                </div>
            </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold;">
                 Pre-trained Large Language Models Use Fourier Features to Compute Addition
            </div>
                <div style="margin-left: 15px;">
                Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia

            <br>
            <i>Neurips, 2024</i><br>
            <a href="https://arxiv.org/abs/2406.03445"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2406.03445"><b>pdf</b></a>
                </div>
            </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold;">
                Transformers Learn Low Sensitivity Functions: Investigations and Implications
            </div>
                <div style="margin-left: 15px;">
                Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan
            <br>
            <i>ICLR, 2025</i><br>
            <a href="https://arxiv.org/abs/2403.06925"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2403.06925"><b>pdf</b></a>
                </div>
            </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold;">
                Discovering Data Structures: Nearest Neighbor Search and Beyond
            </div>
                <div style="margin-left: 15px;">
                    Omar Salemohamed, Laurent Charlin, Shivam Garg, Vatsal Sharan, Gregory Valiant
            <br>
            <i>arXiv, 2024</i><br>
            <a href="https://arxiv.org/abs/2411.03253"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2411.03253"><b>pdf</b></a>
                </div>
            </p></li>
            
            
    </ul>
            
            <a data-toggle="collapse" href="#collapseTf"><b>Some more work along these lines</b></a>
            
            <div class="collapse space justify" id="collapseTf">
                
                <ul>
                    
                    <li><p><div style="font-size:15px;font-weight:bold;">
                         Mitigating Simplicity Bias in Deep Learning for Improved OOD Generalization and Robustness
                    </div>
                        <div style="margin-left: 15px;">
                        Bhavya Vasudeva, Kameron Shahabi, Vatsal Sharan
                    <br>
                    <i>TMLR, 2024</i><br>
                    <a href="https://arxiv.org/abs/2310.06161"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2310.06161.pdf"><b>pdf</b></a>
                        </div>
                    </p></li>
                   

                    <li><p><div style="font-size:15px;font-weight:bold">One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks
                        Learning</div>
                        <div style="margin-left: 15px;">
                        Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal Sharan, Xin Wang, Qiuyi Zhang<br>
                        <i>ICLR, 2021</i><br>
                        <a data-toggle="collapse" href="#collapseExample_deep"><b>abstract</b></a></b></a> | <a href="https://arxiv.org/abs/2103.15261"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2103.15261.pdf"><b>pdf</b></a> | <a href="https://iclr.cc/virtual/2021/poster/3253"><b>video</b></a>
                        </div>
                        </p></li>
                    
                    </ul>
                
                
            </div>
            
        
            
            <div style="height: 12px;"></div>
            
            <h4><em>A multi-group perspective to go beyond loss minimization in ML</em></h4>
            

            
            <div style="height: 8px;"></div>
            
            Minimizing some loss function on average across all datapoints is the dominant paradigm in ML, but applications of ML in societal systems often involve more complex considerations. Different individuals participating in the system may have their own loss functions, it may not be possible to make decisions for these individuals in isolation, and we may care about the model’s behavior on various groups of individuals and not just on average across all of them. Some of my work here uses a multigroup perspective --- which examines the model's predictions on a large number of groups within the population --- to provide solutions to some of the above problems.
            <div style="height: 8px;"></div>
            
            Here is a <a href="https://www.birs.ca/events/2024/5-day-workshops/24w5308/videos/watch/202410221336-Sharan.html"><b>talk</b></a> which covers some of this work (<a href="docs/vatsal_banff.pdf"><b>slides</b></a>).
            <br>
        
        <ul>

        
        <li><p><div style="font-size:15px;font-weight:bold;">
           When is Multicalibration Post-Processing Necessary?
        </div>
            <div style="margin-left: 15px;">
            Dutch Hansen, Siddartha Devic, Preetum Nakkiran, Vatsal Sharan
        <br>
        <i>Neurips, 2024</i><br>
        <a href="https://arxiv.org/abs/2406.06487"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2406.06487"><b>pdf</b></a>
            </div>
        </p></li>
        
        <li><p><div style="font-size:15px;font-weight:bold;">
            Stability and Multigroup Fairness in Ranking with Uncertain Predictions
        </div>
            <div style="margin-left: 15px;">
            Siddartha Devic, Aleksandra Korolova, David Kempe, Vatsal Sharan
        <br>
        <i>ICML, 2024</i><br>
        <i>Non-archival at FORC, 2024</i><br>
        <a href="https://arxiv.org/abs/2402.09326"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2402.09326"><b>pdf</b></a> | <a href="https://www.youtube.com/watch?v=qCFDnBHfGtk"><b>video</b></a>
            </div>
        </p></li>
        
        <li><p><div style="font-size:15px;font-weight:bold">Omnipredictors
        </div>
            <div style="margin-left: 15px;">
        Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, Udi Wieder
        <br>
        <i>ITCS, 2022</i><br>
        <a href="https://arxiv.org/pdf/2109.05389"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2109.05389.pdf"><b>pdf</b></a>
            </div>
        </p></li>
        
        </ul>
        
        <a data-toggle="collapse" href="#collapseFair"><b>Some more work along these lines</b></a>
        
        <div class="collapse space justify" id="collapseFair">
            
            <ul>
                
                <li><p><div style="font-size:15px;font-weight:bold;">
                     Optimal Multiclass U-Calibration Error and Beyond
                </div>
                    <div style="margin-left: 15px;">
                    Haipeng Luo, Spandan Senapati, Vatsal Sharan

                <br>
                <i>Neurips, 2024</i><br>
                <a href="https://arxiv.org/abs/2405.19374"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2405.19374"><b>pdf</b></a>
                    </div>
                </p></li>
                
                <li><p><div style="font-size:15px;font-weight:bold">Fairness in Matching under Uncertainty
                </div>
                    <div style="margin-left: 15px;">
                    Siddartha Devic, David Kempe, Vatsal Sharan, Aleksandra Korolova
                <br>
                <i>ICML, 2023</i><br>
                <i>Non-archival at EAAMO, 2023</i><br>
                <a href="https://arxiv.org/abs/2302.03810"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2302.03810.pdf"><b>pdf</b></a>
                    </div></p></li>
                
                <li><p><div style="font-size:15px;font-weight:bold">KL Divergence Estimation with Multi-group Attribution
                </div>
                    <div style="margin-left: 15px;">
                    Parikshit Gopalan, Nina Narodytska, Omer Reingold, Vatsal Sharan, Udi Wieder
                <br>
                <i>arXiv, 2022</i><br>
                <a href="https://arxiv.org/abs/2202.13576"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2202.13576.pdf"><b>pdf</b></a>
                    </div>
                </p></li>
                
                <li><p><div style="font-size:15px;font-weight:bold">Multicalibrated Partitions for Importance Weights
                </div>
                    <div style="margin-left: 15px;">
                Parikshit Gopalan, Omer Reingold, Vatsal Sharan, Udi Wieder
                <br>
                <i>ALT, 2022</i><br>
                <a href="https://arxiv.org/abs/2103.05853"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2103.05853.pdf"><b>pdf</b></a>
                    </div>
                </p></li>
                
                
                </ul>
            
        </div>
        
        <div style="height: 12px;"></div>
        
        
        <h4>
            <em>Memory as a lens to understand efficient learning and optimization</em>
          </h4>
        
        <div style="height: 8px;"></div>
        
        Classical learning theory  mainly focuses on the number of operations performed by the algorithm as the proxy for the algorithm’s running time. However, since growth in the available processing power has outpaced the growth in the available memory by many orders of magnitude, memory rather than compute has become the primary bottleneck in many applications. Despite this, and even though memory has traditionally been one of the most fundamental computational resources in theoretical computer science, very little is known about the role of memory in solving learning tasks. My work investigates the role of memory in learning, and if memory could be a useful discerning factor to provide a clear separation between 'efficient' and 'expensive' techniques.
        <div style="height: 8px;"></div>
        
        Here is a <a href="https://www.youtube.com/watch?v=--vC_MXcbFA"><b>talk</b></a> which covers some of this work (<a href="docs/vatsal_ipam.pdf"><b>slides</b></a>).
        
        <ul>
            
            <li><p><div style="font-size:15px;font-weight:bold">Efficient Convex Optimization Requires Superlinear Memory
            </div>
                <div style="margin-left: 15px;">
            Annie Marsden, Vatsal Sharan, Aaron Sidford, Gregory Valiant
            <br>
            <i>COLT, 2022</i><br>
            <i><b> Best Paper Award </b> </i><br>
            <i>Invited to IJCAI 2023 Sister Conference Notable Paper Track</i><br>
            <i>JACM, 2024</i><br>
            <a href="https://arxiv.org/abs/2203.15260"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2203.15260.pdf"><b>pdf</b></a>
                </div>
            </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold">Big-Step-Little-Step: Efficient Gradient Methods for Objectives with Multiple Scales
            </div>
                <div style="margin-left: 15px;">
            Jonathan Kelner, Annie Marsden, Vatsal Sharan, Aaron Sidford, Gregory Valiant, Honglin Yuan
            <br>
            <i>COLT, 2022</i><br>
            <a href="https://arxiv.org/abs/2111.03137"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2111.03137.pdf"><b>pdf</b></a>
                </div>
            </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold">Memory-Sample Tradeoffs for Linear Regression with Small Error</div>
                <div style="margin-left: 15px;">
                    Vatsal Sharan, Aaron Sidford, Gregory Valiant<br>
                <i>STOC, 2019</i><br>
                <a data-toggle="collapse" href="#collapseExample4"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1904.08544.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1904.08544"><b>arXiv</b></a>
                </div>
                </p></li>
            
            <div class="collapse space justify" id="collapseExample4">
                What is the role of memory in continuous optimization and learning? Are there inherent trade-offs between the available memory and the data requirement? Is it possible to achieve the sample complexity of second-order optimization methods with significantly less memory? We raise these questions, and show the first nontrivial lower bounds for linear regression with super-linear memory: we show that there is a gap between the sample complexity of algorithms with quadratic memory and that of any algorithm with sub-quadratic memory. <br><br>
            </div>
            
            
            </ul>
        
        <a data-toggle="collapse" href="#collapseMemory"><b>Some more work along these lines</b></a>
        
        <div class="collapse space justify" id="collapseMemory">
            
            <ul>
                
                <li><p><div style="font-size:15px;font-weight:bold">NeuroSketch: A Neural Network Method for Fast and Approximate Evaluation of Range Aggregate Queries
                </div>
                    <div style="margin-left: 15px;">
                    Sepanta Zeighami, Vatsal Sharan, Cyrus Shahabi
                <br>
                <i>SIGMOD, 2023</i><br>
                <a href="https://arxiv.org/abs/2211.10832"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2211.10832.pdf"><b>pdf</b></a>
                    </div>
                </p></li>
                
                <li><p><div style="font-size:15px;font-weight:bold">Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data</div>
                    <div style="margin-left: 15px;">
                        Vatsal Sharan, Kai Sheng Tai, Peter Bailis, Gregory Valiant</i><br>
                    <i>ICML, 2019 </i><br>
                    <a data-toggle="collapse" href="#collapseExample3"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1706.08146.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1706.08146"><b>arXiv</b></a> | <a href="https://github.com/kaishengtai/compressed-factorization"><b>code</b></a> | <a data-toggle="collapse" href="#video3"><b>spotlight video</b></a>
                    </div>
                    </p></li>
                
                <div class="collapse space justify" id="collapseExample3">
                    What learning algorithms can be run directly on compressively-sensed data? If we compress a high dimensional matrix via a random projection, then what is the relationship between the factors of the original matrix and the factors of the compressed matrix? While it is well-known that random projections preserve a number of geometric properties of a dataset, this work shows that random projections can also preserve certain solutions to non-convex, NP-Hard problems like non-negative matrix factorization---both in theory and in practice.<br><br>
                </div>
                
                <div class="collapse space justify" id="video3">
                    Jump to 53:20 in this <a href="https://slideslive.com/38917770/optimization">video</a>.<br>
                </div>
                
                <li><p><div style="font-size:15px;font-weight:bold">Efficient Anomaly Detection via Matrix Sketching</div>
                    <div style="margin-left: 15px;">
                        Vatsal Sharan, Parikshit Gopalan, Udi Wieder</i><br>
                    <i>NeurIPS, 2018</i><br>
                    <a data-toggle="collapse" href="#collapseExample6"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1804.03065.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1804.03065"><b>arXiv</b></a>
                    </div>
                    </p></li>
                
                <div class="collapse space justify" id="collapseExample6">
                    PCA based anomaly scores are commonly used for finding anomalies in high-dimensional data. The naive algorithms for computing these scores explicitly compute the PCA of the covariance matrix which uses space quadratic in the dimensionality of the data. Using matrix sketching tools and new matrix perturbation inequalities, we give the first streaming algorithms for computing these scores that use space that is linear or sublinear in the dimension.<br><br>
                </div>
                
                <li><p><div style="font-size:15px;font-weight:bold">Moment-Based Quantile Sketches for Efficient High Cardinality Aggregation Queries</div>
                    <div style="margin-left: 15px;">
                        Edward Gan, Jialin Ding, Kai Sheng Tai, Vatsal Sharan, Peter Bailis</i><br>
                    <i>VLDB, 2018</i><br>
                    <a data-toggle="collapse" href="#collapseExample8"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1803.01969.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1803.01969"><b>arXiv</b></a> | <a href="https://github.com/stanford-futuredata/msketch"><b>code</b></a>
                    </div>
                    </p></li>
                
                <div class="collapse space justify" id="collapseExample8">
                    We propose a compact and efficiently mergeable sketch for answering quantile queries on a dataset. This data structure, which we refer to as the moments sketch, operates with a small memory footprint (200 bytes) and achieves computationally efficient (50ns) merges by tracking only a set of summary statistics, notably the sample moments.<br><br>
                </div>
                
                <li><p><div style="font-size:15px;font-weight:bold">Prediction with a Short Memory</div>
                    <div style="margin-left: 15px;">
                        Vatsal Sharan, Sham Kakade, Percy Liang, Gregory Valiant</i><br>
                    <i>STOC, 2018</i><br>
                    <a data-toggle="collapse" href="#collapseExample9"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1612.02526.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1612.02526"><b>arXiv</b></a> | <a href="https://theorydish.blog/2017/11/29/prediction-with-a-short-memory/"><b>blog</b></a> | <a href="https://www.youtube.com/watch?v=-Lhmr9vqjrE&t=5s"><b>video</b></a>
                    </div>
                    </p></li>
                
                <div class="collapse space justify" id="collapseExample9">
                    When is it necessary to remember significant information from the distant past to make good predictions about the future in sequential prediction tasks? How do we consolidate and reference memories about the past in order to predict the future? This work studies these questions, and the findings are perhaps counterintuitive: we show that a simple (Markov) model which bases its predictions on only the most recent observations and the statistics of short windows, can  achieve small error on average with respect to a large class of sequences, such as those generated by Hidden Markov Models (HMMs). This shows that long-term memory may not be necessary for making good predictions on average, even on sequences generated by complex processes that have long-range dependencies.<br><br>
                </div>
                
                <li><p><div style="font-size:15px;font-weight:bold">Sketching Linear Classifiers over Data Streams</div>
                    <div style="margin-left: 15px;">
                        Kai Sheng Tai, Vatsal Sharan, Peter Bailis, Gregory Valiant</i><br>
                    <i>SIGMOD, 2018</i><br>
                    <a data-toggle="collapse" href="#collapseExample10"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1711.02305.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1711.02305"><b>arXiv</b></a> | <a href="https://github.com/stanford-futuredata/wmsketch"><b>code</b></a>
                    </div>
                    </p></li>
                
                <div class="collapse space justify" id="collapseExample10">
                    We introduce a new sub-linear space sketch---the Weight-Median Sketch---for learning compressed linear classifiers over data streams while supporting the efficient recovery of large-magnitude weights in the classifier. The Weight-Median Sketch adopts the core data structure used in the Count-Sketch, but, instead of sketching counts, it captures sketched gradient updates to the model parameters. We establishes recovery guarantees for our sketch and demonstrate empirical improvements in memory-accuracy trade-offs over alternative memory-budgeted methods. <br><br>
                </div>
                
                </ul>
            
        </div>
        
        <div style="height: 12px;"></div>
        
        <h4>
            <em>Learning with limited (or synthetic) data</em>
          </h4>
        
        <div style="height: 8px;"></div>
        
        Modern ML techniques are data-hungry, but data is an expensive resource. Current ML applications have brought to light several interesting information-theoretic questions around learning with limited data. Some of my work provides a formulation to study the basic statistical task of synthetically augmenting a given set of samples, and explores notions of regularization --- a classical technique for data-efficient learning whose role still remains mysterious in deep learning.
        
        <ul>

        <li><p><div style="font-size:15px;font-weight:bold;">
                  Proper Learnability and the Role of Unlabeled Data
            </div>
                <div style="margin-left: 15px;">
                Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng
            <br>
            <i>ALT, 2025</i><br>
            <a href="https://arxiv.org/abs/2502.10359"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2502.10359"><b>pdf</b></a>
                </div> </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold;">
                  Regularization and Optimal Multiclass Learning
            </div>
                <div style="margin-left: 15px;">
                Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng
            <br>
            <i>COLT, 2024</i><br>
            <a href="https://arxiv.org/abs/2309.13692"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2309.13692.pdf"><b>pdf</b></a>
                </div> </p></li>
            
            <li><p><div style="font-size:15px;font-weight:bold">On the Statistical Complexity of Sample Amplification
            </div>
                <div style="margin-left: 15px;">
            Brian Axelrod, Shivam Garg, Yanjun Han, Vatsal Sharan, Gregory Valiant
            <br>
            <i>Annals of Statistics, 2024</i><br>
            <a href="https://arxiv.org/abs/2201.04315"><b>arXiv</b></a> | <a href="https://vatsalsharan.github.io/annals_sample_amplification.pdf"><b>preprint pdf</b></a>
                </div>
            </p></li>
            
        </ul>
        
        <a data-toggle="collapse" href="#collapseData"><b>Some more work along these lines</b></a>
        
        <div class="collapse space justify" id="collapseData">
            
            <ul>
                
               

               <li><p><div style="font-size:15px;font-weight:bold;">
                   Transductive Sample Complexities Are Compact
               </div>
                   <div style="margin-left: 15px;">
                   Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng

               <br>
               <i>Neurips, 2024</i><br>
               <a href="https://arxiv.org/abs/2402.10360"><b>arXiv</b></a> | <a href="https://arxiv.org/pdf/2402.10360"><b>pdf</b></a>
                   </div>
               </p></li>
               
               <li><p><div style="font-size:15px;font-weight:bold;">
                   Open Problem: Can Local Regularization Learn All Multiclass Problems?
               </div>
                   <div style="margin-left: 15px;">
                   Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng
               <br>
               <i>Open Problem @ COLT, 2024</i><br>
                <a href="https://jasilis.com/documents/papers/open-problem-local-regularization.pdf"><b>pdf</b></a>
                   </div></p></li>
               
               <li><p><div style="font-size:15px;font-weight:bold">Sample Amplification: Increasing Dataset Size even when Learning is Impossible</div>
                   <div style="margin-left: 15px;">
                       Brian Axelrod, Shivam Garg, Vatsal Sharan, Gregory Valiant<br>
                <i>ICML, 2020 </i><br>
                <a data-toggle="collapse" href="#collapseExample_sample"><b>abstract</b></a> | <a href="https://arxiv.org/pdf/1904.12053.pdf"><b>pdf</b></a> | <a href="https://arxiv.org/abs/1904.12053"><b>arXiv</b></a> | <a data-toggle="collapse" href="#video2"><b>video</b></a>
                   </div>
                </p></li>

                <div class="collapse space justify" id="collapseExample_sample">
                  <i>Is learning a distribution always necessary for generating new samples from the distribution?</i> We introduce the problem of "sample amplification": given <i>n</i> independent draws from a distribution, <i>D</i> , to what extent is it possible to output a set of <i>m > n </i>  datapoints that are indistinguishable from <i>m</i>  i.i.d. draws from <i>D</i> ? Curiously, we show that nontrivial amplification is often possible in the regime where <i>n</i>  is too small to learn <i>D</i>  to any nontrivial accuracy.<br><br>
               </div>
               
               <div class="collapse space justify" id="video2">
                   Jump to 50:00 (or topic #27) in this <a href="https://slideslive.com/38922033/machine-learning-with-guarantees-1">video</a>.<br>
               </div>
                
                </ul>
            
        </div>
            
            
            <div style="height: 12px;"></div>

	  </section>
      
      <section id="students">
        <h3>Students</h3>

        I am very lucky to advise an <a href="docs/group_photo_2025.jpg">amazing group of students,</a> including the following amazing Ph.D. students:<br><br>

        <ul>

        <li> <a href="https://sid.devic.us/"><b>Siddartha Devic</b></a> (co-advised with <a href="https://www.korolova.com/"><b>Aleksandra Korolova</b>) </li>
        <li> <a href="https://estija.github.io/"><b>Bhavya Vasudeva</b></a> </li>
        <li> <a href="http://www.jasilis.com/"><b>Julian Asilis</b></a> </li>
        <li> <a href="https://sites.google.com/usc.edu/deqingfu"><b>Deqing Fu</b></a> (co-advised with <a href="https://robinjia.github.io/"><b>Robin Jia</b></a>) </li>
        <li> Devansh Gupta</b></a> (co-advised with <a href="https://sites.usc.edu/razaviyayn/"><b>Meisam Razaviyayn</b></a>) </li>
        <li> Spandan Senapati</b></a> (co-advised with <a href="https://haipeng-luo.net/index.html"><b>Haipeng Luo</b></a>) </li>
        <li> <a href="https://kevinzhoutianyi.github.io/"><b>Tianyi Zhou </b></a> (co-advised with <a href="https://robinjia.github.io/"><b>Robin Jia</b></a>)  </li>

        </ul>
        <div style="height: 8px;"></div>
        
        And the following amazing undergrad/Masters students:<br><br>
        
        <ul>

        <li>    Dutch Hansen </li>
        <li>    Anish Jayant </li>
         <li>   Jung Whan Lee </li>
         <li>   You Qi Huang (SURE program intern in Summer'23, mentored by Bhavya Vasudeva) </li>
         <li>   Natalie Abreu (graduated in Fall'23, now Ph.D. student at Harvard) </li>
         <li>   Aditya Prased (graduated in Fall'24, now Ph.D. student at the University of Chicago) </li>
         <li>   Kameron Shahabi (graduated in Fall'24, now Ph.D. student at the University of Washington) </li>
         <li>   Qilin Ye (joint with Robin Jia, graduated in Fall'24, now M.S. student at Duke) </li>
         <li>   Devin Martin (SURE program intern in Summer'22, mentored by Bhavya Vasudeva) </li>

        </ul>
      
      <section id="teaching">
      <h3>Teaching</h3><br>
      
      <ul>
          <li> <a href="https://vatsalsharan.github.io/spring24.html">CSCI 567: Machine Learning (Spring 2024) </a>
          <li> <a href="https://vatsalsharan.github.io/fall23.html">CSCI 699: Theory of Machine Learning (Fall 2023) </a>
          <li> CSCI 699: Seminar on Computational Perspectives on the Frontiers of ML (Spring 2023)
          <li> <a href="https://vatsalsharan.github.io/fall22.html">CSCI 567: Machine Learning (Fall 2022) </a>
          <li> <a href="https://vatsalsharan.github.io/fall21.html">CSCI 699: Theory of Machine Learning (Fall 2021) </a>
	 </ul>
     
     <br><br><br>
     
	</div>
      </div>
    </div>
  
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.3.min.js"></script>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    
  </body>
</html>
