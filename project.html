<!DOCTYPE html>
<html lang="en">
  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="css/starter-template.css" rel="stylesheet">
        <title>CS699: Theory of ML</title>
  </head>

<body>
<div class="container">

	    	<div class="row">
            <div class="col-lg-12">
                <h1 class="page-header" style="color:#B03A2E;">CSCI 699: Theory of Machine Learning, Class presentations
                </h1>
            </div>
        </div>

<p>

<h3 style="color:#1F618D;"></h3>

<h3 style="color:#1F618D;">General instructions</h3>
<br>
Student presentations are the most important part of the course. Their goal is two-fold:
<br><br>
<ol>
    <li> Give you a taste of ML theory research, especially many interesing areas and topic which we do not cover in class.
    <li> Give you an opportunity to practice the skill of giving talks, which is an extremely important part of your PhD training (but is often overlooked)
</ol>

Each presentation will be about 35 minute long, with 15 minutes for questions and discussion, for a total of about 50 minutes. For the presentation you can either use slides, use the tablet I use in class or, if you prefer, the blackboard. To ensure all presentations are high-quality, one week before the presentations, you are asked to meet with me to review your slides/material and your preparation. Therefore, you should try to be ready with your presentation a week before it is scheduled. This preparation review will be worth 10% of your grade, the actual in-class presentation will be worth 20%.
<br><br>
Some general advide regarding the talk:
<br><br>
<ol>
    <li> Try to do one practice talk before your talk, with any of your friends, classmates etc. if possible. One of the surest ways of giving good talks is to get feedback and be reasonably well prepared.
        <li> You are totally not expected to cover everything in your paper! In fact, most likely you will only have time to cover one or two main results. Try to convey one or more key insights or takeaways instead of a lot of technical details.
        <li> On a related note, even if the paper is notation-heavy, it does not mean your talk has to be so. Try to use a minimum amount of notation to convey what you want to say. It is common to make simplification in talks even if that means sacrificing generality or even rigorousness.
        <li> To combine the above two points, when you read the paper and plan your presentations, try to identify one or two key ideas that you want to present. You should try to present at least one theoretical result in a self-sufficient way in the presentation, including the proof or proof sketch (you'll probably only have time to do this for one theoretical result). This would mean that you might have to present the result in the most simple and cleanest setting, trying to avoid all extra details and jargon. Try to also relate the presentation to material covered in class wherever possible.
        <li> I encourage you to watch these extremely nice short videos by Uri Alon on how to give a good talk: <a href=" https://www.youtube.com/watch?v=5OFAhBw0OXs">[1] </a> <a href=" https://www.youtube.com/watch?v=Fg_Bn8k0uaQ">[2] </a> <a href=" https://www.youtube.com/watch?v=zYsHxNiPg7M">[3] </a> <a href=" https://www.youtube.com/watch?v=OhnSSjQCm4c">[4] </a> <a href=" https://www.youtube.com/watch?v=FYkdzZgCX4M">[5] </a> <a href=" https://www.youtube.com/watch?v=y-fhwNa7fnQ">[6] </a>.
</ol>
<br>
Some remarks regarding the papers:
<br><br>
<ol>
    <li> You are <b>not</b> required to understand every single detail of the papers! Instead, try to focus on the key ideas/messages. It is fine that you only skim some parts of the paper, as long as you spend time in carefully reading and understanding some other main parts. Importantly, if your paper is long, you only need to read about 20-30 pages for the presentation, enough to get some key ideas and present them. If your project report will be on the same paper, you can read a bit more for the final report, or read a bit more of some other related paper, but around 30 pages is still sufficient.
<li> If you need further help understanding your paper, feel free to ask on Piazza or schedule an appointment with me.
    </ol>
    

<h3 style="color:#1F618D;">Schedule and papers</h3>
<br>
<ul>

<li> <b> Monday Nov 1 </b>

    <ul>
    
    <li> Slot 1 (starting around 11am): Fatih Erdem Kizilkaya [Computational/statistical tradeoffs]
<ul>

<li> <a href="https://arxiv.org/pdf/1806.07508.pdf">Reducibility and Computational Lower Bounds for Problems with Planted Sparse Structure </a>
    <li>Review with instructor: Monday Oct 25, 1pm-1:45pm
</ul>
    
    </ul>
    
    
<li> <b> Wednesday Nov 3 </b>
<ul>
    <li> Slot 1:  Chandra Sekhar Mukherjee [Computational/statistical tradeoffs]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/2009.06107.pdf">Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent </a>
            <li> Review with instructor: Wednesday Oct 27, 2pm-2:45pm
    
    </ul>
        <li> Slot 2: Jesse Zhang [Stability for understanding generalization]

            <ul>
            <li> <a href="https://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf7.pdf">Learnability, Stability and Uniform Convergence </a>
                <li> Review with instructor: Wednesday Oct 27, 2:45pm-3:30pm

    </ul>
</ul>

<li> <b> Monday  Nov 8 </b>

<ul>
    <li> Slot 1: Ta-Yang Wang [Generalization for deep neural networks]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/1611.03530.pdf">Understanding deep learning requires rethinking generalization </a> & <a href="https://arxiv.org/pdf/1412.6614.pdf">In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning </a>
            <li> Review with instructor: Monday Nov 1, 1pm-1:45pm
    
    </ul>
        <li> Slot 2: Emir Ceyani [PAC-Bayes, Generalization for deep neural networks]

            <ul>
            <li> <a href="https://arxiv.org/pdf/1707.09564.pdf">A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks</a>, & <a href="https://arxiv.org/pdf/1706.08498.pdf">Spectrally-normalized margin bounds for neural networks </a> if possible
                <li> Review with instructor: Monday Nov 1, 1:45pm-2:30pm

    </ul>
</ul>

<li> <b> Wednesday  Nov 10 </b>

<ul>
    <li> Slot 1: Sophie Hsu [Generalization for deep neural networks]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/1802.05296.pdf">Stronger generalization bounds for deep nets via a compression approach </a>
            <li> Review with instructor: Wednesday Nov 3, 1pm-1:45pm
    
    </ul>
        <li> Slot 2: Di Zhang [Generalization for deep neural networks]

            <ul>
            <li> <a href="http://vtaly.net/papers/F-interpolate-0619.pdf">Does Learning Require Memorization? A Short Tale about a Long Tail </a> (also see <a href="http://vtaly.net/papers/FZ_Infl_mem.pdf">Discovering the Long Tail via Influence Estimation</a>)
<li> Review with instructor: Wednesday Nov 3, 1:45pm-2:30pm

    </ul>
</ul>

<li> <b> Monday  Nov 15 </b>

<ul>
    <li> Slot 1: Grace Zhang [Generalization for deep neural networks]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/1712.09203.pdf">Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations </a>
<li> Review with instructor: Monday Nov 8, 1pm-1:45pm
    
    </ul>
        <li> Slot 2: Jiahao Wen [Optimization for deep neural networks]

            <ul>
            <li> <a href="https://arxiv.org/pdf/2103.13462.pdf">Why Do Local Methods Solve Nonconvex Problems? </a>
<li> Review with instructor: Monday Nov 8, 1:45pm-2:30pm

    </ul>
</ul>


<li> <b> Wednesday  Nov 17 </b>

<ul>
    <li> Slot 1:  Berk TÄ±naz [Out of distribution generalization]
        
        <ul>
        <li> <a href="https://www.alexkulesza.com/pubs/adapt_mlj10.pdf">A theory of learning from different domains </a>
            <li> Review with instructor: Wednesday Nov 10, 2pm-2:45pm
    
    </ul>
        <li> Slot 2: Bhavya Vasudeva [Out of distribution generalization]

            <ul>
            <li> <a href="https://arxiv.org/pdf/2010.15775.pdf">Understanding the failure modes of out-of-distribution generalization </a>
                <li> Review with instructor: Wednesday Nov 10, 2:45pm-3:30pm

    </ul>
</ul>


<li> <b> Monday  Nov 22 </b>

<ul>
    <li> Slot 1:  Navid Hashemi [Adversarial robustness]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/2005.10190.pdf">Feature Purification: How Adversarial Training Performs Robust Deep Learning </a>
<li> Review with instructor: Monday Nov 15, 1pm-1:45pm
    
    </ul>
        <li> Slot 2: Neel Patel [Robust ML]

            <ul>
            <li> <a href="https://arxiv.org/pdf/1911.05911.pdf">Recent Advances in Algorithmic High-Dimensional Robust Statistics</a> (also see <a href="http://people.csail.mit.edu/moitra/docs/cacmrobust.pdf">Robustness Meets Algorithms</a>)
                <li> Review with instructor: Monday Nov 15, 1:45pm-2:30pm

    </ul>
</ul>

<li> <b> Monday  Nov 29 </b>

<ul>
    <li> Slot 1:  Sid Devic [Semi-supervised learning]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/2010.03622.pdf">Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data </a>
<li> Review with instructor: Monday Nov 22, 1pm-1:45pm
    
    </ul>
        <li> Slot 2: Zhengqi Wu [Beyond iid data]

            <ul>
            <li> <a href="https://arxiv.org/pdf/1911.03605.pdf">Worst-Case Analysis for Randomly Collected Data</a>
<li> Review with instructor: Monday Nov 22, 1:45pm-2:30pm

    </ul>
</ul>

<li> <b> Wednesday  Dec 1 </b>

<ul>
    <li> Slot 1:  Ali Omrani [Fairness]
        
        <ul>
        <li> <a href="https://arxiv.org/pdf/1104.3913.pdf">Fairness Through Awareness </a>
<li> Review with instructor: Tuesday Nov 23, 10am-10:45am
    
    </ul>
        <li> Slot 2: Yingxiao Ye [Fairness]

            <ul>
            <li> <a href="https://arxiv.org/pdf/1609.05807.pdf">Inherent Trade-Offs in the Fair Determination of Risk Scores</a>
                <li> Review with instructor: Tuesday Nov 23, 10:45am-11:30am

    </ul>
</ul>

<h3 style="color:#1F618D;">Project report</h3>

The report summarizes and distills the main results of your assigned papers, other papers that you chose to read, or the research you chose to do. You should aim to read around 30 pages, so if your paper is longer than this you don't need to read all of it.  The final report has to be written in Latex and should be 7-8 pages long, excluding references. Please use <a href="/lecture_notes/project_template.zip">this LaTex template</a> based on the NeurIPS format. You can use the following format as a guide but you don't need to follow this strictly. For instance, if you're including some research that you did, you will probably modify the outline accordingly. 

<br><br>
<ol>

<li> <b> Abstract: </b> A short abstract, summarizing the entire survey.
<li> <b> Introduction: </b> Introduce the main topic of you assigned paper(s). Try to put it into the context of general learning theory, and explain how it relates to topics covered in our lectures (to the extent possible). Briefly mention the high-level results and explain the significance of the results (such as improvement over prior work).
<li> <b> Problem setup: </b> For problem setup, you should describe the problems in detail using necessary notation. Once again, you are not asked to cover everything in the papers, so only describe in detail what you plan to cover in this short survey.
<li> <b> Main results: </b> Describe the main algorithms/theorems. For algorithms, describe what they are doing at each step and what the key idea is behind it. For theorems, after the formal statement, try to explain in words what the statement really means and what the implications are.
<li> <b> Proofs: </b> Try to distill some proofs from the paper and reproduce them in the report. Due to the space limit, most likely you can only fit 1-2 proofs into the report, so pick the ones that you think are most important. If the original proofs are long and complicated, try to break it down into several parts (in the form of lemmas for example), and only present the proofs for some of these parts.
<li> <b> Experiments: </b> If the paper has experiments and you think they are an important component, mention them here.
<li> <b> Conclusion: </b> A short conclusion, highlighting the main message again.
<li> <b> References. </b>

</ol>

<br>

The following two are optional and considered as bonus tasks. You can use up to 1 extra page for each of these two extra components:

<br><br>
<ol>

<li> <b> Open questions: </b> Identify interesting and concrete open questions in the same direction that are not mentioned in the papers already. Mention briefly what you think the potential approaches are to tackle these open questions and/or why you think these are hard problems that require new techniques beyond what the papers present.
<li> <b> More papers in the same direction: </b> Read more related papers of the same topic and include an extended related work section that summarizes what the other papers you read are about and how they are compared to your assigned papers. To find these other papers, you can use the reference list of your assigned papers, use Google Scholar to see which papers cite your assigned papers, or do a general online search of the related topic.

</ol>

<br>

The project should be written in a way such that anyone else in the class who doesn't read the original paper(s) but directly reads the report can still follow along. Try to have this in mind as you write and make the exposition as clean and clear as possible, and have sufficient intution along with the formal details. The report will mainly be evaluated on the quality of the writing and the presentation, and how your understanding of the paper is reflected in it.

<br>

The report is due on <b> Monday December 13th at 10am </b> (corresponding to the final exam time allocated for the class). No extensions or late days are possible for the report, so please plan accordingly.

      <br><br><br>
     
</div>
</div>
</body>
</html>
